{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-28T13:35:31.893786Z",
     "iopub.status.busy": "2021-05-28T13:35:31.893431Z",
     "iopub.status.idle": "2021-05-28T13:35:36.596868Z",
     "shell.execute_reply": "2021-05-28T13:35:36.596106Z",
     "shell.execute_reply.started": "2021-05-28T13:35:31.893710Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import cv2\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# # detect and init the TPU\n",
    "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "\n",
    "# # instantiate a distribution strategy\n",
    "# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T12:54:04.490835Z",
     "iopub.status.busy": "2021-05-28T12:54:04.490451Z",
     "iopub.status.idle": "2021-05-28T12:54:04.498264Z",
     "shell.execute_reply": "2021-05-28T12:54:04.497270Z",
     "shell.execute_reply.started": "2021-05-28T12:54:04.490803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256 #16 * tpu_strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_df=pd.read_csv('../input/chestxray8-dataframe/train_df.csv').drop(['Image Index','Patient ID'],axis=1) #.drop_duplicates('Patient ID','last')\n",
    "# if used drop duplicates then we can only work with those classes : Effusion,Infiltration, Mass, Nodule ,Atelectasis                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df= vanilla_df[vanilla_df['No Finding'] !=1 ]\n",
    "illness_df['Normal']=illness_df['No Finding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df= illness_df[ illness_df['Hernia'] !=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df= illness_df[ illness_df['Pneumonia'] !=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df.drop(['No Finding','Hernia','Pneumonia'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_df.drop(['Hernia','Pneumonia'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_df['Normal']=vanilla_df['No Finding']\n",
    "vanilla_df.drop( ['No Finding'] , axis=1 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df=vanilla_df[ vanilla_df['Normal'] ==1 ].loc[ 0:7200,:] # taking only 4000 images with normal conditions\n",
    "normal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effusion_df=vanilla_df[ vanilla_df['Effusion'] ==1 ].loc[ 0:40000,:] # taking only 4000 images with conditions\n",
    "effusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infiltration_df=vanilla_df[ vanilla_df['Infiltration'] ==1 ].loc[ 0:22000,:] # taking only 3200 images with conditions\n",
    "infiltration_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atelectasis_df=vanilla_df[ vanilla_df['Atelectasis'] ==1 ].loc[ 0:33000,:] # taking only 3100 images with normal conditions\n",
    "atelectasis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df.drop(index= illness_df[illness_df['Effusion']==1].index , axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## thats what we want now, to remove the excess rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df.drop(index= illness_df[illness_df['Infiltration']==1].index , axis=0, inplace=True)\n",
    "illness_df.drop(index= illness_df[illness_df['Atelectasis']==1].index , axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illness_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df= illness_df.append([normal_df,atelectasis_df,infiltration_df,effusion_df])\n",
    "path=balanced_df['FilePath']\n",
    "balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col= ['Cardiomegaly','Emphysema','Effusion','Infiltration',\n",
    "      'Mass','Nodule','Atelectasis','Pneumothorax',\n",
    "      'Pleural_Thickening','Fibrosis','Edema','Consolidation','Normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.drop_duplicates('FilePath', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great now that our dataset is kind of balanced, we can proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df= balanced_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reducing the batch size as SGD consumes a large chunk of memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_gen= tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                                            samplewise_center=True,\n",
    "                                                            samplewise_std_normalization=True,\n",
    "                                                            rotation_range=0.2,\n",
    "                                                            zca_whitening=True,\n",
    "                                                            width_shift_range=0.1,\n",
    "                                                            height_shift_range=0.1,\n",
    "                                                            shear_range=0.0,\n",
    "                                                            zoom_range=0.2,\n",
    "                                                            horizontal_flip=True,\n",
    "                                                            rescale=1/255.,\n",
    "                                                            validation_split=0.1)\n",
    "\n",
    "\n",
    "train_data= tmp_gen.flow_from_dataframe(  dataframe= balanced_df ,\n",
    "                                          directory= None ,\n",
    "                                          x_col='FilePath' ,\n",
    "                                          y_col= col ,\n",
    "                                          class_mode=\"raw\" ,\n",
    "                                          batch_size= BATCH_SIZE ,\n",
    "                                          shuffle= True ,\n",
    "                                          target_size= (224,224),\n",
    "                                          subset=\"training\"\n",
    "                                       )\n",
    "\n",
    "val_data= tmp_gen.flow_from_dataframe(  dataframe= balanced_df ,\n",
    "                                         directory= None ,\n",
    "                                         x_col= 'FilePath' ,\n",
    "                                         y_col= col ,\n",
    "                                         class_mode= \"raw\" ,\n",
    "                                         batch_size= BATCH_SIZE ,\n",
    "                                         shuffle= True ,\n",
    "                                         target_size= (224,224),\n",
    "                                         subset= 'validation'\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T13:36:06.884176Z",
     "iopub.status.busy": "2021-05-28T13:36:06.883824Z",
     "iopub.status.idle": "2021-05-28T13:36:07.321075Z",
     "shell.execute_reply": "2021-05-28T13:36:07.320350Z",
     "shell.execute_reply.started": "2021-05-28T13:36:06.884146Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adadelta, Adagrad\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating index to class dictionary\n",
    "idx_class={i:c for i,c in enumerate(col)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CAM(processed_image, actual_label, layer_name='conv2d_127'):\n",
    "    model_grad = Model( [model.inputs] ,   [model.get_layer(layer_name).output , model.output]  )\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output_values, predictions = model_grad(processed_image)\n",
    "\n",
    "        # watch the conv_output_values\n",
    "        tape.watch(conv_output_values)\n",
    "\n",
    "        ## Use binary cross entropy loss\n",
    "        ## actual_label is 0 if cat, 1 if dog\n",
    "        # get prediction probability of dog\n",
    "        # If model does well, \n",
    "        # pred_prob should be close to 0 if cat, close to 1 if dog\n",
    "        pred_prob = predictions[:,1] # [ batch , (cat_prob , dog_prob) ]\n",
    "        # we tale only one prbability to be able to use binary_crossentropy_loss not sparse_categorical_loss\n",
    "        \n",
    "        # make sure actual_label is a float, like the rest of the loss calculation\n",
    "        actual_label = tf.cast( actual_label , dtype=tf.float32 )\n",
    "        \n",
    "        # add a tiny value to avoid log of 0\n",
    "        smoothing = 0.00001 \n",
    "        \n",
    "        # Calculate loss as binary cross entropy\n",
    "        # we can use tf.keras in that too\n",
    "        # bce = tf.keras.losses.BinaryCrossentropy()\n",
    "        # bce(y_true, y_pred).numpy()\n",
    "\n",
    "\n",
    "        loss = -1 * ( actual_label * tf.math.log(pred_prob + smoothing) + (1 - actual_label) * tf.math.log(1 - pred_prob + smoothing) )\n",
    "        print(f\"binary loss: {loss}\")\n",
    "    \n",
    "    # get the gradient of the loss with respect to the outputs of the last conv layer\n",
    "    grads_values = tape.gradient(loss , conv_output_values)\n",
    "    grads_values = tf.keras.backend.mean(grads_values , axis=(0,1,2)) # mean over batch , hight , width --> num of channels\n",
    "    \n",
    "    conv_output_values = np.squeeze( conv_output_values.numpy() ) # will remove the 1 valued dimention which is the batch  --> (h , w )\n",
    "    grads_values = grads_values.numpy()\n",
    "    print(conv_output_values.shape)\n",
    "    # weight the convolution outputs with the computed gradients\n",
    "    for i in range(128): # num of filter channels\n",
    "        conv_output_values[ : , : , i ] *= grads_values[i] # multiply the gradient of the channels by the channels values\n",
    "    heatmap = np.mean(conv_output_values, axis=-1)# taking the mean over the channels , --> ( h , w )\n",
    "    \n",
    "    heatmap = np.maximum(heatmap, 0) # taking only the positive values\n",
    "    heatmap /= heatmap.max()# regularizing the pixel values\n",
    "    \n",
    "    del model_grad, conv_output_values, grads_values, loss\n",
    "   \n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample():\n",
    "    \n",
    "\n",
    "    images, labels= next(val_data)\n",
    "    sample_image = images[0]  # batch 0 so that returns ( h , w , c) for the image, without the batch dimention\n",
    "    sample_label = labels[0] # takes batch of xs and ys # x= train_data.next() -> x[0].shape -> 32,224,224,3\n",
    "    \n",
    "    sample_image_processed = np.expand_dims(sample_image, axis=0) # adding back the batch dimention\n",
    "    \n",
    "    activations = vis_model.predict(sample_image_processed) # the output of each layer -features-\n",
    "    \n",
    "    pred_label = np.argmax( model.predict(sample_image_processed) , axis=-1 )[0]\n",
    "    pred_label = idx_class[pred_label]\n",
    "    \n",
    "    print(activations[0].shape)\n",
    "    sample_activation = activations[0] [0 , : , : , -1] # taking the first output , for image of batch 0, and for the last layer #16 , --> (h,w)\n",
    "    \n",
    "    sample_activation-=sample_activation.mean()\n",
    "    sample_activation/=sample_activation.std()\n",
    "    \n",
    "    sample_activation *=255\n",
    "    sample_activation = np.clip( sample_activation , 0 , 255 ).astype(np.uint8)\n",
    "    \n",
    "    heatmap = get_CAM(sample_image_processed , sample_label )\n",
    "    heatmap = cv2.resize( heatmap, ( sample_image.shape[0], sample_image.shape[1 ]) )\n",
    "    heatmap = heatmap *255\n",
    "    heatmap = np.clip( heatmap , 0 , 255 ).astype(np.uint8)\n",
    "    heatmap = cv2.applyColorMap( heatmap , cv2.COLORMAP_HOT )\n",
    "    converted_img = sample_image\n",
    "    super_imposed_image = cv2.addWeighted( converted_img, 0.8, heatmap.astype('float32'), 2e-3, 0.0 )\n",
    "    \n",
    "    sample_label = idx_class[np.argmax(sample_label)]\n",
    "    \n",
    "    f,ax = plt.subplots(2,2, figsize=(15,8))\n",
    "\n",
    "    ax[0,0].imshow(sample_image)\n",
    "    ax[0,0].set_title(f\"True label: {sample_label} \\n Predicted label: {pred_label}\")\n",
    "    ax[0,0].axis('off')\n",
    "    \n",
    "    ax[0,1].imshow(sample_activation)\n",
    "    ax[0,1].set_title(\"Random feature map\")\n",
    "    ax[0,1].axis('off')\n",
    "    \n",
    "    ax[1,0].imshow(heatmap)\n",
    "    ax[1,0].set_title(\"Class Activation Map\")\n",
    "    ax[1,0].axis('off')\n",
    "    \n",
    "    ax[1,1].imshow(super_imposed_image)\n",
    "    ax[1,1].set_title(\"Activation map superimposed\")\n",
    "    ax[1,1].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "  \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we can see i'm only refining the last layer only, 14k params to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  # load the base VGG16 model\n",
    "  base_model = load_model('../input/chet-xray-encoder-model/encoder_model.h5')\n",
    "  \n",
    "  # build on top of AE\n",
    "  #output = layers.GlobalAveragePooling2D()(base_model.output)\n",
    "  output=layers.Flatten()(base_model.output)\n",
    "  output = layers.BatchNormalization()(output)\n",
    "  output = layers.Dense(64, activation='relu')(output)\n",
    "  output = layers.Dropout(0.4)(output)\n",
    "#   output = layers.Dense(32, activation='relu')(output)\n",
    "#   output = layers.BatchNormalization()(output)\n",
    "  output = layers.Dense( len(col) , activation='sigmoid')(output)\n",
    "\n",
    "  # set the inputs and outputs of the model\n",
    "  model = Model( base_model.input , output )\n",
    "\n",
    "  # freeze the earlier layers and leave the last 4 layers to train\n",
    "    \n",
    "  for layer in base_model.layers[:]:\n",
    "       layer.trainable=False\n",
    "\n",
    "  # choose the optimizer\n",
    "  #optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  # configure the model for training\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', \n",
    "                optimizer= 'adam',#RMSprop( 0.001 , momentum=0.98 ), #Adam(0.004 ), #Adadelta(),\n",
    "                metrics=[tf.keras.metrics.AUC()])\n",
    "  \n",
    "  # display the summary\n",
    "  model.summary()\n",
    "  \n",
    "  return model\n",
    "\n",
    "model=build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***let's Plot the outputs before model training***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all the layers for which you want to visualize the outputs and store it in a list\n",
    "outputs = [ layer.output for layer in model.layers[1:] ] # all layers except the input layer\n",
    "\n",
    "# Define a new model that generates the above output\n",
    "vis_model = Model(model.input , outputs)\n",
    "\n",
    "# store the layer names we are interested in\n",
    "layer_names = []\n",
    "for layer in outputs:\n",
    "    layer_names.append( layer.name.split(\"/\")[0] )\n",
    "\n",
    "    \n",
    "print(\"Layers that will be used for visualization: \")\n",
    "print(layer_names)\n",
    "# Choose an image index to show, or leave it as None to get a random image\n",
    "activations = show_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our call backs\n",
    "cb= tf.keras.callbacks.ModelCheckpoint( \"my_model.h5\" , save_best_only=True  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history= model.fit( train_data , validation_data= val_data , \n",
    "                    epochs= 50 , callbacks= [cb] \n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a non completed training model to continue the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T13:36:27.925265Z",
     "iopub.status.busy": "2021-05-28T13:36:27.924922Z",
     "iopub.status.idle": "2021-05-28T13:36:30.984532Z",
     "shell.execute_reply": "2021-05-28T13:36:30.983744Z",
     "shell.execute_reply.started": "2021-05-28T13:36:27.925236Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiating the model in the strategy scope creates the model on the TPU\n",
    "model = load_model('../input/nih-13classes-pretrained-model/13-class-model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history= model.fit( train_data , validation_data= val_data , \n",
    "                    epochs= 15 , callbacks= [cb] \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "fig.suptitle('Train vs Valid')\n",
    "\n",
    "ax1.plot(range(12), model.history.history['loss'],color='b', label='loss')\n",
    "ax1.plot(range(12), model.history.history['val_loss'],color='r', label='val_loss')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "\n",
    "ax2.plot(range(12), model.history.history['auc'], label='auc')\n",
    "ax2.plot(range(12), model.history.history['val_auc'], label='val_auc')\n",
    "ax2.set_ylabel('auc')\n",
    "\n",
    "ax2.set_xlabel('Epochs')\n",
    "\n",
    "\n",
    "ax2.set_ylim([0,1])\n",
    "ax1.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train all the layers to see an improvements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model.layers :\n",
    "    l.trainable= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "history= model.fit( train_data , validation_data= val_data , \n",
    "                    epochs= 15 , callbacks= [cb] \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model= Model(inputs= model.inputs , outputs= model.output )\n",
    "\n",
    "sgd_model.compile(loss='binary_crossentropy', \n",
    "                optimizer= tf.keras.optimizers.SGD(0.005 , 0.9) ,#RMSprop( 0.001 , momentum=0.98 ), #Adam(0.004 ), #Adadelta(),\n",
    "                metrics=[tf.keras.metrics.AUC() , 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.Model.save(sgd_model, './my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history= sgd_model.fit( train_data , validation_data= val_data , \n",
    "                    epochs= 20 , callbacks= [cb] \n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.Model.save(sgd_model, './my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting after returning to train back and forth, which will show nothing useful "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "fig.suptitle('Train vs Valid')\n",
    "\n",
    "ax1.plot(range(12), model.history.history['loss'],color='b', label='loss')\n",
    "ax1.plot(range(12), model.history.history['val_loss'],color='r', label='val_loss')\n",
    "ax1.set_ylabel('Loss')\n",
    "\n",
    "\n",
    "ax2.plot(range(12), model.history.history['auc'], label='auc')\n",
    "ax2.plot(range(12), model.history.history['val_auc'], label='val_auc')\n",
    "ax2.set_ylabel('auc')\n",
    "\n",
    "ax2.set_xlabel('Epochs')\n",
    "\n",
    "\n",
    "ax2.set_ylim([0,1])\n",
    "ax1.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all the layers for which you want to visualize the outputs and store it in a list\n",
    "outputs = [ layer.output for layer in model.layers[1:] ] # all layers except the input layer\n",
    "\n",
    "# Define a new model that generates the above output\n",
    "vis_model = Model(model.input , outputs)\n",
    "\n",
    "# store the layer names we are interested in\n",
    "layer_names = []\n",
    "for layer in outputs:\n",
    "    layer_names.append( layer.name.split(\"/\")[0] )\n",
    "\n",
    "    \n",
    "print(\"Layers that will be used for visualization: \")\n",
    "print(layer_names)\n",
    "# Choose an image index to show, or leave it as None to get a random image\n",
    "activations = show_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn\n",
    "test_generator= val_data.next()\n",
    "y_predict =np.argmax( model.predict(test_generator[0]),axis=1)\n",
    "\n",
    "#tn, fp, fn, tp = np.max( confusion_matrix( test_generator.labels , y_predict ) , axis=1)\n",
    "matrix=confusion_matrix(np.argmax(test_generator[1], axis=1) , y_predict)\n",
    "print(matrix)\n",
    "# Confusion matrix Plotting\n",
    "import seaborn as sns\n",
    "#classes=['covid', 'normal', 'pnumonia']\n",
    "sns.heatmap(matrix, annot=True, xticklabels=col, yticklabels=col ,cmap='Blues')#YlGnBu_r or Blues or twilight_shifted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "#2- setting the path of the image\n",
    "path='../input/nawwar/1.jpeg'\n",
    "#3- uploading the image into a variable\n",
    "\n",
    "img= image.load_img( path , target_size=( 224,224 ) )\n",
    "# don't forget the target size the model is expecting\n",
    "#4- processing the image variable to suit the model\n",
    "\n",
    "x= image.img_to_array( img )\n",
    "x= np.expand_dims( x , axis=0 )\n",
    "images= np.vstack( [x] )\n",
    "\n",
    "plt.imshow(img) # to show the image\n",
    "# to predict the image\n",
    "print('Class is: ', idx_class[np.argmax(model.predict(x))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(image):\n",
    "    \n",
    "    sample_image = image  # batch 0 so that returns ( h , w , c) for the image, without the batch dimention\n",
    "    #sample_label = label # takes batch of xs and ys # x= train_data.next() -> x[0].shape -> 32,224,224,3\n",
    "    \n",
    "    sample_image_processed = np.expand_dims(sample_image, axis=0) # adding back the batch dimention\n",
    "    \n",
    "    activations = vis_model.predict(sample_image_processed) # the output of each layer -features-\n",
    "    \n",
    "    pred_label = np.argmax( model.predict(sample_image_processed) , axis=-1 )[0]\n",
    "    pred_label = idx_class[pred_label]\n",
    "    \n",
    "    print(activations[0].shape)\n",
    "    sample_activation = activations[0] [0 , : , : , :3] # taking the first output , for image of batch 0, and for the last layer #16 , --> (h,w)\n",
    "    \n",
    "    sample_activation-=sample_activation.mean()\n",
    "    sample_activation/=sample_activation.std()\n",
    "    \n",
    "    sample_activation *=255\n",
    "    sample_activation = np.clip( sample_activation , 0 , 255 ).astype(np.uint8)\n",
    "    \n",
    "    f,ax = plt.subplots(1,2, figsize=(15,8))\n",
    "\n",
    "    ax[0].imshow(sample_image)\n",
    "    ax[0].set_title(f\"Predicted label: {pred_label}\")\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    ax[1].imshow(sample_activation)\n",
    "    ax[1].set_title(\"Random feature map\")\n",
    "    ax[1].axis('off')\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "  \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "#2- setting the path of the image\n",
    "path='../input/nawwar/1.jpeg'\n",
    "#3- uploading the image into a variable\n",
    "\n",
    "img= image.load_img( path , target_size=( 224,224 ) )\n",
    "# don't forget the target size the model is expecting\n",
    "#4- processing the image variable to suit the model\n",
    "\n",
    "x= image.img_to_array( img )\n",
    "\n",
    "c=classify(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T13:39:15.723540Z",
     "iopub.status.busy": "2021-05-28T13:39:15.723221Z",
     "iopub.status.idle": "2021-05-28T13:39:15.727525Z",
     "shell.execute_reply": "2021-05-28T13:39:15.726597Z",
     "shell.execute_reply.started": "2021-05-28T13:39:15.723510Z"
    }
   },
   "outputs": [],
   "source": [
    "lite_model=tf.lite.TFLiteConverter.from_keras_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T13:39:20.692636Z",
     "iopub.status.busy": "2021-05-28T13:39:20.692325Z",
     "iopub.status.idle": "2021-05-28T13:39:20.702080Z",
     "shell.execute_reply": "2021-05-28T13:39:20.700928Z",
     "shell.execute_reply.started": "2021-05-28T13:39:20.692607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.lite.python.lite.TFLiteKerasModelConverterV2 at 0x7fabb7fe9e10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-28T13:49:59.549219Z",
     "iopub.status.busy": "2021-05-28T13:49:59.548855Z",
     "iopub.status.idle": "2021-05-28T13:50:07.356320Z",
     "shell.execute_reply": "2021-05-28T13:50:07.355339Z",
     "shell.execute_reply.started": "2021-05-28T13:49:59.549187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-28 13:50:00.433270: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-05-28 13:50:00.436606: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-05-28 13:50:02.495269: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-05-28 13:50:02.496208: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-05-28 13:50:02.500582: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:02.500990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-05-28 13:50:02.501040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-05-28 13:50:02.502859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-05-28 13:50:02.502940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-05-28 13:50:02.504729: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-05-28 13:50:02.505083: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-05-28 13:50:02.507079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-05-28 13:50:02.508186: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-05-28 13:50:02.512234: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-28 13:50:02.512393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:02.512834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:02.513234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-05-28 13:50:02.513508: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-05-28 13:50:02.513798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:02.514216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-05-28 13:50:02.514255: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-05-28 13:50:02.514296: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-05-28 13:50:02.514325: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-05-28 13:50:02.514348: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-05-28 13:50:02.514370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-05-28 13:50:02.514393: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-05-28 13:50:02.514415: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-05-28 13:50:02.514439: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-28 13:50:02.514541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:02.514997: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:02.515328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-05-28 13:50:03.083295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-28 13:50:03.083362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-05-28 13:50:03.083383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-05-28 13:50:03.083667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:03.084191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:03.084598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:03.084958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 363 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "2021-05-28 13:50:03.085338: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-05-28 13:50:04.546732: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "I0528 13:50:06.043746 139650853345088 builder_impl.py:775] Assets written to: /tmp/tmp_k_6z_6f/assets\n",
      "2021-05-28 13:50:06.194509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.195162: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2021-05-28 13:50:06.195545: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session\n",
      "2021-05-28 13:50:06.196208: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.196708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-05-28 13:50:06.196822: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-05-28 13:50:06.196900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-05-28 13:50:06.196935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-05-28 13:50:06.197050: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-05-28 13:50:06.197089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-05-28 13:50:06.197118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-05-28 13:50:06.197147: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-05-28 13:50:06.197190: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-28 13:50:06.197340: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.197821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.198306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-05-28 13:50:06.201908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-28 13:50:06.201972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-05-28 13:50:06.202013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-05-28 13:50:06.202180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.202810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.203327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 363 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "2021-05-28 13:50:06.203404: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-05-28 13:50:06.203701: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000185000 Hz\n",
      "2021-05-28 13:50:06.207118: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:928] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.006ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0ms.\n",
      "\n",
      "I0528 13:50:06.585280 139650853345088 lite.py:619] Using new converter: If you encounter a problem please file a bug. You can opt-out by setting experimental_new_converter=False\n",
      "2021-05-28 13:50:06.629094: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored output_format.\n",
      "2021-05-28 13:50:06.629149: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:319] Ignored drop_control_dependency.\n",
      "2021-05-28 13:50:06.676961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.677397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
      "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
      "2021-05-28 13:50:06.677469: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.2\n",
      "2021-05-28 13:50:06.677540: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-05-28 13:50:06.677567: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-05-28 13:50:06.677592: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-05-28 13:50:06.677615: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-05-28 13:50:06.677638: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-05-28 13:50:06.677666: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-05-28 13:50:06.677696: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-05-28 13:50:06.677820: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.678282: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.678608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2021-05-28 13:50:06.678665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-28 13:50:06.678688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2021-05-28 13:50:06.678704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2021-05-28 13:50:06.678835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.679245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-28 13:50:06.679584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 363 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
      "2021-05-28 13:50:06.679624: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    }
   ],
   "source": [
    "!tflite_convert  --keras_model_file=../input/nih-13classes-pretrained-model/13-class-model.h5  --output_file=./litemodel.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
